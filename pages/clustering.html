<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Clustering Analysis</title>
    <link rel="stylesheet" href="../css/styles.css">  <!-- Global Styles -->
    <link rel="stylesheet" href="../css/clustering.css">  <!-- Page-Specific Styles -->
</head>
<body>

    <!-- Navbar -->
    <nav class="navbar">
        <div class="container">
            <a href="../index.html" class="logo">Diabetes Project</a>
            <ul class="nav-links">
                <li><a href="introduction.html">Introduction</a></li>
                <li><a href="data_prep.html">Data Prep & EDA</a></li>
                <li><a href="models.html">ML Models</a></li>
                <li><a href="pca.html">PCA</a></li>
                <li><a href="clustering.html" class="active">Clustering</a></li>
                <li><a href="arm.html" >ARM</a></li>
                <li><a href="conclusions.html">Conclusions</a></li>
                <li><a href="contact.html">Contact</a></li>
            </ul>
        </div>
    </nav>

    <!-- Hero Section -->
    <header class="hero">
        <div class="hero-content">
            <h1>Clustering Analysis</h1>
            <p>Uncovering Patterns in Diabetes Data</p>
        </div>
    </header>

    <!-- Clustering Content -->
    <section class="content-section">
        <div class="container">

            <div class="info-box">
                <h2>What is Clustering?</h2>
                <p>Clustering is an unsupervised learning technique used to group similar data points. We use three clustering methods:</p>
                <ul>
                    <li><strong>K-Means Clustering</strong> - Divides data into k-groups based on similarity.</li>
                    <li><strong>Hierarchical Clustering</strong> (HClust) - Builds a hierarchy of clusters.</li>
                    <li><strong>DBSCAN</strong> - Identifies clusters based on density and detects outliers.</li>
                </ul>
                <img src="../images/clustering 1.png" alt="K-Means Clustering Results" class="zoomable">
                
                <div class="info-box">
                    <h2>Clustering and Distance Metrics - Brief Overview</h2>
                    <p>Clustering is an unsupervised learning method that groups similar data points based on a predefined similarity measure.</p>
                    
                    <h3>Clustering Distance Metrics</h3>
                    <p>Distance metrics like Euclidean distance, Cosine similarity, and Manhattan distance are explored to determine the most suitable clustering approach.</p>
                    <img src="../images/clustering_distance_metrics.png" alt="Clustering Distance Metrics" class="zoomable">
                    
                    <h3>How Clustering is Used in This Project</h3>
                    <p>Three clustering techniques K-Means, Hierarchical Clustering, and DBSCAN are applied to group diabetes data based on patterns and relationships.</p>
                    <img src="../images/clustering 2.png" alt="K-Means Clustering Results" class="zoomable">
                </div>
            </div>

            <div class="info-box">
                <h2>GitHub Repository</h2>
                <a href="https://github.com/rohanjain11/diabetes-project/blob/5accfb73176251959901a9f46baed249780c84cb/Models%20ipynb/clustering.ipynb" target="_blank" class="github-link">
                    View Clustering Code on GitHub â†—
                </a>
            </div>
            <div class="info-box">
                <h2>Dataset Used</h2>
                <p>We use the cleaned diabetes dataset for clustering. Below is a preview:</p>
                <img src="../images/cleaned_data.png" alt="Cleaned Dataset" class="zoomable">
                <a href="../data/diabetes_cleaned.csv" download class="download-btn">Download Cleaned Dataset</a>
            </div>

             <!-- Data Preprocessing Section -->
            <div class="info-box">
                <h2>Data Preprocessing Steps</h2>

                <h3>1. Load the Dataset</h3>
                <p>The cleaned diabetes dataset is loaded before clustering.</p>
                <img src="../images/load_dataset.png" alt="Load Dataset" class="zoomable">

                <h3>2. Dropping Non-Numeric Columns</h3>
                <p>Only numerical features are retained for clustering.</p>
                <img src="../images/drop_non_numeric.png" alt="Dropping Non-Numeric Columns" class="zoomable">

                <h3>3. Standardizing the Columns</h3>
                <p>StandardScaler is applied to normalize feature values.</p>
                <img src="../images/standardized_data.png" alt="Standardizing Data" class="zoomable">
            </div>

            <div class="info-box">
                <h2>K-Means Clustering</h2>
                <p>Below is the K-Means clustering visualization with the optimal number of clusters:</p>             
                <img src="../images/kmeans_clusters.png" alt="K-Means Clustering Results" class="zoomable">
                <p>
                    The K-Means clustering visualization displays data points grouped into three distinct clusters 
                    using Principal Component Analysis (PCA) for dimensionality reduction. Each color represents a 
                    separate cluster identified by the K-Means algorithm, highlighting natural groupings in the dataset. 
                    The x-axis and y-axis correspond to the first two principal components, preserving the most 
                    significant variance in the data. This clustering approach helps in identifying underlying patterns 
                    and similarities between data points, aiding in better data segmentation and analysis.
                </p>   
                <h3>Silhouette Score Analysis</h3>
                <p>Silhouette Score was used to determine the best number of clusters.</p>
                <img src="../images/silhouette_score.png" alt="Silhouette Score" class="zoomable">
                <p>
                    The silhouette score plot helps determine the optimal number of clusters (k) for K-Means clustering. 
                    The y-axis represents the silhouette score, which measures how well-defined the clusters are, 
                    while the x-axis represents different values of k. Higher scores indicate better clustering. 
                    As observed, the silhouette score is highest for k=2 and k=3, suggesting that these values 
                    provide the most meaningful clustering structure in the dataset. The sharp decline in score 
                    beyond k=3 indicates that adding more clusters may not significantly improve the separation.
                </p>
                
                <p style="text-align: center;">Selected 3 Optimal k Values: [2, 3, 4]</p>
                <h3>K-Means on different cluster</h3>
                <img src="../images/kmeans_clusters_2.png" alt="K-Means Clustering Results for different clusters." class="zoomable">
                <p>
                    The visualization displays the results of K-Means clustering applied to the dataset with different 
                    values of k (number of clusters). The leftmost plot (k=2) separates the data into two broad groups, 
                    but lacks finer distinctions. The middle plot (k=3) provides a more meaningful separation, 
                    aligning well with the natural structure in the data. The rightmost plot (k=4) introduces an 
                    additional cluster, potentially leading to over-segmentation. Based on the silhouette score 
                    analysis, k=3 appears to be the most optimal choice, balancing cohesion and separation.
                </p>
                
            </div>

            <div class="info-box">
                <h2>Hierarchical Clustering</h2>
                <p>Hierarchical clustering dendrogram is shown below:</p>
                <img src="../images/hierarchical_dendrogram.png" alt="Hierarchical Clustering Dendrogram" class="zoomable">
                <p>
                    The hierarchical clustering dendrogram visually represents the hierarchical relationships among data points.
                    Each merge represents clusters combining based on their similarity, with the y-axis (distance) showing how 
                    dissimilar two clusters are before merging. The large height of the final merges indicates that three 
                    distinct clusters are optimal, which aligns with previous clustering results. This approach helps in 
                    understanding natural data groupings and is useful when the number of clusters is unknown.
                </p>
                
                <h2>Hierarchical clustering using cosine similarity.</h2>
                <img src="../images/hierarchical_clusters.png" alt="Hierarchical Clustering Results" class="zoomable">
                <p>
                    This dendrogram represents hierarchical clustering based on cosine similarity, a metric particularly useful 
                    for high-dimensional data. Instead of Euclidean distance, cosine similarity measures the angle between 
                    vectors, making it effective for text data and feature-rich datasets. The clustering structure indicates 
                    that three distinct groups emerge naturally. Cosine similarity-based clustering is valuable when dealing 
                    with non-linear relationships and sparse data, ensuring meaningful groupings in complex datasets.
                </p>
                
            </div>

            <div class="info-box">
                <h2>DBSCAN Clustering</h2>
                <p>DBSCAN clustering was applied to detect dense regions and outliers.</p>
                <img src="../images/dbscan_clusters.png" alt="DBSCAN Clustering Results" class="zoomable">
                <p>
                    The DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm was used to identify 
                    dense clusters and detect outliers in the dataset. Unlike K-Means, DBSCAN does not require specifying the 
                    number of clusters beforehand. The plot visualizes clusters using two principal components, where different 
                    colors represent distinct clusters. Points labeled as "-1" indicate noise or outliers that do not belong to 
                    any cluster. This method is particularly useful for datasets with irregular shapes and varying densities, 
                    making it a powerful tool for anomaly detection and pattern recognition.
                </p>
                
                
            </div>

            <div class="info-box">
                <h2>Performance Comparison</h2>
                <table class="comparison-table">
                    <tr>
                        <th>Method</th>
                        <th>Performance</th>
                        <th>Scalability</th>
                        <th>Interpretability</th>
                    </tr>
                    <tr>
                        <td>K-Means</td>
                        <td>Fast & Efficient</td>
                        <td>Scalable</td>
                        <td>Good for spherical clusters</td>
                    </tr>
                    <tr>
                        <td>Hierarchical</td>
                        <td>Slower for large data</td>
                        <td>Less scalable</td>
                        <td>Useful for hierarchy</td>
                    </tr>
                    <tr>
                        <td>DBSCAN</td>
                        <td>Handles noise well</td>
                        <td>Not good for high dimensions</td>
                        <td>Great for irregular clusters</td>
                    </tr>
                </table>
            </div>
            <div class="info-box">
                <h2>Clustering Conclusion</h2>
            
                <h3>K-Means Clustering:</h3>
                <ul>
                    <li>The <strong>silhouette score analysis</strong> helped identify the optimal number of clusters, with three clusters performing well.</li>
                    <li>The <strong>2D PCA projection</strong> of K-Means clustering shows well-separated groups, indicating effective partitioning.</li>
                    <li>The method works well for spherical clusters but may struggle with more complex, non-linear structures.</li>
                </ul>
            
                <h3>Hierarchical Clustering:</h3>
                <ul>
                    <li>The <strong>dendrogram visualization</strong> indicates the hierarchical relationships between data points.</li>
                    <li>Using <strong>cosine similarity</strong> improved cluster separation compared to Euclidean distance.</li>
                    <li>However, hierarchical clustering is <strong>computationally expensive</strong> for large datasets and may not scale efficiently.</li>
                </ul>
            
                <h3>DBSCAN Clustering:</h3>
                <ul>
                    <li>DBSCAN effectively identifies <strong>dense clusters and outliers</strong>, making it suitable for datasets with varying density.</li>
                    <li>The presence of outliers (noise points labeled as <strong>-1</strong>) indicates that the dataset contains regions of lower density.</li>
                    <li>However, <strong>parameter tuning (epsilon and min_samples)</strong> is crucial for obtaining meaningful clusters.</li>
                </ul>
            
                <h3>Overall Insights:</h3>
                <ul>
                    <li><strong>K-Means</strong> is efficient for well-separated, compact clusters but struggles with varying density.</li>
                    <li><strong>Hierarchical Clustering</strong> provides interpretability but is computationally expensive for large datasets.</li>
                    <li><strong>DBSCAN</strong> handles noise and irregular cluster shapes well but requires careful parameter selection.</li>
                    <li>Each clustering method has its strengths and limitations, and the choice of technique depends on <strong>dataset characteristics</strong> and the problem at hand.</li>
                </ul>
            </div>
            
            

        </div>
    </section>

    <script src="../js/script.js"></script>
</body>
</html>
